{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red wine quality prediction\n",
    "\n",
    "Plot different hyperparameters against the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/winequality-red.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the weight by counting the number of samples in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39133627 0.39133627 0.39133627 ... 0.4177116  0.39133627 0.4177116 ]\n"
     ]
    }
   ],
   "source": [
    "weight = np.ones(data.shape[0])\n",
    "freq = 1 / data['quality'].value_counts()\n",
    "for i in freq.index:\n",
    "    weight[data['quality'] == i] = freq[i]\n",
    "weight = weight / weight.mean()\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "1. Check missing data - none\n",
    "2. Drop the irrelevant columns - unecessary in this case\n",
    "3. Ensure data types are correct\n",
    "3. Encode the categorical columns\n",
    "4. Normalize the continuous columns\n",
    "5. Put back the categorical and continuous columns together\n",
    "6. Split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only continuous columns exist\n",
    "X_all = data.drop('quality', axis = 1).values\n",
    "X_all = (X_all - X_all.mean()) / X_all.std()\n",
    "y_all = data['quality'].values\n",
    "\n",
    "# add a weight column so that it will be split up properly with the train and test\n",
    "X_all = np.concatenate([X_all, weight.reshape(-1,1)], axis = 1)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# convert stuff to torch tensors so they can be used in PyTorch training\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "# separate the weight column\n",
    "weight_train = X_train[:, -1]\n",
    "X_train = X_train[:,:-1]\n",
    "weight_test = X_test[:, -1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_sizes, p):\n",
    "        super().__init__()\n",
    "\n",
    "        layerlist = []\n",
    "        for i in hidden_sizes:\n",
    "            layerlist.append(nn.Linear(in_size,i)) \n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            in_size = i\n",
    "        layerlist.append(nn.Linear(hidden_sizes[-1], out_size))\n",
    "\n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(input, target, weight = None, type = 'mse', v = 0.04):\n",
    "    if weight is None:\n",
    "        weight = torch.ones(input.shape[0])\n",
    "\n",
    "    if type == 'mse':\n",
    "        return (weight * (input - target)**2).mean()\n",
    "    elif type == 'mae':\n",
    "        return (weight * torch.abs(input - target)).mean()\n",
    "    elif type == 'kge':\n",
    "        x = torch.hstack([input, target.reshape(-1,1)]).T\n",
    "        cost = (torch.corrcoef(x)[0,1] - 1) ** 2 + \\\n",
    "               (torch.mean(input)/torch.mean(target) - 1) ** 2 + \\\n",
    "               (torch.std(input)/(torch.std(target)+1e-12) -  1) ** 2\n",
    "        return cost\n",
    "    elif type == 'corr':\n",
    "        x = torch.hstack([input, target.reshape(-1,1)]).T\n",
    "        cost = - torch.corrcoef(x)[0,1]\n",
    "        if torch.isnan(cost):\n",
    "            cost = 0\n",
    "        return cost\n",
    "    elif type == 'corr1':\n",
    "        x = torch.hstack([input, target.reshape(-1,1)]).T\n",
    "        cost = - torch.corrcoef(x)[0,1]\n",
    "        if torch.isnan(cost):\n",
    "            cost = 0\n",
    "        else:\n",
    "            cost = cost + v * ( torch.abs(input.max()-target.max()) + torch.abs(input.min()-target.min()) )\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training function\n",
    "\n",
    "Use KGE\n",
    "\n",
    "Tune: layer sizes, drop out rates, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(33)\n",
    "\n",
    "def train_ann(config):\n",
    "    # number of times to train the model\n",
    "    epochs = 450\n",
    "\n",
    "    # feed the data into the model by batches\n",
    "    train_dataloader = DataLoader([(X_train[i,:], weight_train[i], y_train[i]) \\\n",
    "                                for i in range(X_train.shape[0])],\n",
    "                                batch_size=512, shuffle=True)\n",
    "\n",
    "    # define the model\n",
    "    model = ANNModel(X_train.shape[1], 1, [config['h1'], config['h2'], config['h3']], \n",
    "                     p = config['p'])\n",
    "\n",
    "    # define the optimizer\n",
    "    sampler = WeightedRandomSampler(weight_train, \n",
    "                                    num_samples=len(weight_train), replacement=True)    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        for j, (train_x, train_w, train_y) in enumerate(train_dataloader):\n",
    "            # make a prediction\n",
    "            y_pred = model(train_x)\n",
    "            # calculate the loss and record it\n",
    "            loss = custom_loss(y_pred, train_y, None, 'kge')\n",
    "            # set the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            # calculate the gradients\n",
    "            loss.backward()\n",
    "            # let the optimizer update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation loop; model.eval() is not necessary here\n",
    "        with torch.no_grad():\n",
    "            y_val = model(X_test)\n",
    "            val_loss = custom_loss(y_val, y_test, None, 'kge')\n",
    "            corr = - custom_loss(y_val, y_test, None, 'corr')\n",
    "\n",
    "        # save the training progress\n",
    "        with tune.checkpoint_dir(epochs) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        # tune will log the keyword arguments\n",
    "        tune.report(loss=float(val_loss.numpy()), corr=float(corr.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-10 15:13:08 (running for 00:01:15.36)<br>Memory usage on this node: 10.6/15.6 GiB<br>Using AsyncHyperBand: num_stopped=24\n",
       "Bracket: Iter 16.000: -0.9419326186180115 | Iter 8.000: -1.1430011987686157 | Iter 4.000: -1.3892616033554077 | Iter 2.000: -1.7664637565612793 | Iter 1.000: -2.057917833328247<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/3.25 GiB heap, 0.0/1.62 GiB objects<br>Current best trial: afdf0_00016 with loss=0.3125200867652893 and parameters={'h1': 1000, 'h2': 100, 'h3': 10, 'p': 0.5640899995873223, 'lr': 0.05428159675295064}<br>Result logdir: C:\\Users\\ywang254\\ray_results\\train_ann_2022-02-10_15-11-52<br>Number of trials: 24/24 (24 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  h1</th><th style=\"text-align: right;\">  h2</th><th style=\"text-align: right;\">  h3</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">        p</th><th style=\"text-align: right;\">      loss</th><th style=\"text-align: right;\">      corr</th><th style=\"text-align: right;\">  training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ann_afdf0_00000</td><td>TERMINATED</td><td>127.0.0.1:8736 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.0191169  </td><td style=\"text-align: right;\">0.614545 </td><td style=\"text-align: right;\">  0.738137</td><td style=\"text-align: right;\"> 0.283611 </td><td style=\"text-align: right;\">                  30</td></tr>\n",
       "<tr><td>train_ann_afdf0_00001</td><td>TERMINATED</td><td>127.0.0.1:7868 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.00222082 </td><td style=\"text-align: right;\">0.652407 </td><td style=\"text-align: right;\">  2.02311 </td><td style=\"text-align: right;\"> 0.0569661</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00002</td><td>TERMINATED</td><td>127.0.0.1:14332</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.00167596 </td><td style=\"text-align: right;\">0.0319525</td><td style=\"text-align: right;\">  0.512829</td><td style=\"text-align: right;\"> 0.420803 </td><td style=\"text-align: right;\">                  30</td></tr>\n",
       "<tr><td>train_ann_afdf0_00003</td><td>TERMINATED</td><td>127.0.0.1:9064 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.000530558</td><td style=\"text-align: right;\">0.720718 </td><td style=\"text-align: right;\">  2.11939 </td><td style=\"text-align: right;\"> 0.0162458</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00004</td><td>TERMINATED</td><td>127.0.0.1:652  </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.000154682</td><td style=\"text-align: right;\">0.197581 </td><td style=\"text-align: right;\">  2.11975 </td><td style=\"text-align: right;\">-0.0600025</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00005</td><td>TERMINATED</td><td>127.0.0.1:9012 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.000160184</td><td style=\"text-align: right;\">0.156845 </td><td style=\"text-align: right;\">  1.78258 </td><td style=\"text-align: right;\"> 0.139655 </td><td style=\"text-align: right;\">                  30</td></tr>\n",
       "<tr><td>train_ann_afdf0_00006</td><td>TERMINATED</td><td>127.0.0.1:18500</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.00149593 </td><td style=\"text-align: right;\">0.858185 </td><td style=\"text-align: right;\">  3.39765 </td><td style=\"text-align: right;\">-0.0566335</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00007</td><td>TERMINATED</td><td>127.0.0.1:8004 </td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.00136774 </td><td style=\"text-align: right;\">0.345923 </td><td style=\"text-align: right;\">  1.67701 </td><td style=\"text-align: right;\"> 0.154121 </td><td style=\"text-align: right;\">                   8</td></tr>\n",
       "<tr><td>train_ann_afdf0_00008</td><td>TERMINATED</td><td>127.0.0.1:8984 </td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.00567673 </td><td style=\"text-align: right;\">0.0257508</td><td style=\"text-align: right;\">  0.509442</td><td style=\"text-align: right;\"> 0.568205 </td><td style=\"text-align: right;\">                  30</td></tr>\n",
       "<tr><td>train_ann_afdf0_00009</td><td>TERMINATED</td><td>127.0.0.1:21648</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.0150571  </td><td style=\"text-align: right;\">0.336074 </td><td style=\"text-align: right;\">  0.520445</td><td style=\"text-align: right;\"> 0.422887 </td><td style=\"text-align: right;\">                  30</td></tr>\n",
       "<tr><td>train_ann_afdf0_00010</td><td>TERMINATED</td><td>127.0.0.1:9800 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.0691479  </td><td style=\"text-align: right;\">0.22279  </td><td style=\"text-align: right;\">219.655   </td><td style=\"text-align: right;\">-0.107495 </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00011</td><td>TERMINATED</td><td>127.0.0.1:10540</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.000328302</td><td style=\"text-align: right;\">0.441915 </td><td style=\"text-align: right;\">  1.82749 </td><td style=\"text-align: right;\"> 0.166718 </td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>train_ann_afdf0_00012</td><td>TERMINATED</td><td>127.0.0.1:9788 </td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.000935614</td><td style=\"text-align: right;\">0.846267 </td><td style=\"text-align: right;\">  3.17685 </td><td style=\"text-align: right;\">-0.0940733</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00013</td><td>TERMINATED</td><td>127.0.0.1:7008 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.000126563</td><td style=\"text-align: right;\">0.553424 </td><td style=\"text-align: right;\">  2.07163 </td><td style=\"text-align: right;\">-0.0248995</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>train_ann_afdf0_00014</td><td>TERMINATED</td><td>127.0.0.1:13036</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.000112144</td><td style=\"text-align: right;\">0.118504 </td><td style=\"text-align: right;\">  2.09272 </td><td style=\"text-align: right;\"> 0.026745 </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00015</td><td>TERMINATED</td><td>127.0.0.1:2996 </td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.0312177  </td><td style=\"text-align: right;\">0.653221 </td><td style=\"text-align: right;\">  1.91591 </td><td style=\"text-align: right;\"> 0.0654678</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>train_ann_afdf0_00016</td><td>TERMINATED</td><td>127.0.0.1:16608</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.0542816  </td><td style=\"text-align: right;\">0.56409  </td><td style=\"text-align: right;\">  0.31252 </td><td style=\"text-align: right;\"> 0.499764 </td><td style=\"text-align: right;\">                  30</td></tr>\n",
       "<tr><td>train_ann_afdf0_00017</td><td>TERMINATED</td><td>127.0.0.1:10932</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.00247509 </td><td style=\"text-align: right;\">0.860092 </td><td style=\"text-align: right;\">  2.5715  </td><td style=\"text-align: right;\"> 0.0503328</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00018</td><td>TERMINATED</td><td>127.0.0.1:16068</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.00042443 </td><td style=\"text-align: right;\">0.421584 </td><td style=\"text-align: right;\">  2.0135  </td><td style=\"text-align: right;\"> 0.0140223</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00019</td><td>TERMINATED</td><td>127.0.0.1:20572</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.0264013  </td><td style=\"text-align: right;\">0.674446 </td><td style=\"text-align: right;\">  1.76646 </td><td style=\"text-align: right;\"> 0.0554802</td><td style=\"text-align: right;\">                   2</td></tr>\n",
       "<tr><td>train_ann_afdf0_00020</td><td>TERMINATED</td><td>127.0.0.1:11376</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.000577441</td><td style=\"text-align: right;\">0.923478 </td><td style=\"text-align: right;\">  2.35429 </td><td style=\"text-align: right;\"> 0.0840912</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00021</td><td>TERMINATED</td><td>127.0.0.1:9408 </td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">0.0020162  </td><td style=\"text-align: right;\">0.0540992</td><td style=\"text-align: right;\">  4.35402 </td><td style=\"text-align: right;\"> 0.18188  </td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00022</td><td>TERMINATED</td><td>127.0.0.1:19020</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">  10</td><td style=\"text-align: right;\">0.000499359</td><td style=\"text-align: right;\">0.962169 </td><td style=\"text-align: right;\">  3.80678 </td><td style=\"text-align: right;\">-0.0817717</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "<tr><td>train_ann_afdf0_00023</td><td>TERMINATED</td><td>127.0.0.1:1928 </td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\"> 100</td><td style=\"text-align: right;\">0.000326742</td><td style=\"text-align: right;\">0.867289 </td><td style=\"text-align: right;\">  3.25723 </td><td style=\"text-align: right;\">-0.0723701</td><td style=\"text-align: right;\">                   1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 15:13:08,215\tINFO tune.py:626 -- Total run time: 75.61 seconds (75.33 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'h1': 1000, 'h2': 100, 'h3': 10, 'p': 0.5640899995873223, 'lr': 0.05428159675295064}\n",
      "Best trial final validation loss: 0.3125200867652893\n",
      "Best trial final validation Pearson correlation: 0.499764084815979\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"h1\": tune.sample_from(lambda _: 10**np.random.randint(1,4)),\n",
    "    \"h2\": tune.sample_from(lambda _: 10**np.random.randint(1,4)),\n",
    "    \"h3\": tune.sample_from(lambda _: 10**np.random.randint(1,4)),\n",
    "    \"p\": tune.sample_from(lambda _: np.random.uniform(0,1)),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "# Implements the Async Successive Halving\n",
    "# https://arxiv.org/abs/1810.05934\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=30, # maximum time units per trial\n",
    "    time_attr = \"training_iteration\", # the time unit\n",
    "    grace_period=1,\n",
    "    reduction_factor=2) # the factor by which the number of trials per configuration is\n",
    "#                         increased each time, while the top 1 / reduction_factor configurations\n",
    "#                         are retained\n",
    "\n",
    "# make a console/jupyter notebook reporter\n",
    "#reporter = CLIReporter(\n",
    "#    metric_columns=[\"loss\", \"corr\", \"training_iteration\"])\n",
    "reporter = JupyterNotebookReporter(\n",
    "    overwrite = True,\n",
    "    metric_columns=[\"loss\", \"corr\", \"training_iteration\"])\n",
    "\n",
    "result = tune.run(\n",
    "    train_ann,\n",
    "    config=config,\n",
    "    metric=\"loss\", # tells what metric to optimize\n",
    "    mode=\"min\", # tells the scheduler to minimize the metric\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    num_samples=24, # number of times to sample from the hyperparameter space\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter,\n",
    "    checkpoint_at_end=True) # save checkpoint at the end of each trial\n",
    "\n",
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"avg\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "       best_trial.last_result[\"loss\"]))\n",
    "print(\"Best trial final validation Pearson correlation: {}\".format(\n",
    "    best_trial.last_result[\"corr\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the final outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.6, 0.1, 'Pearson corr = 0.565422')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the best model\n",
    "best_trained_model = ANNModel(X_train.shape[1], 1, [best_trial.config['h1'], \n",
    "                              best_trial.config['h2'], best_trial.config['h3']], \n",
    "                              p = best_trial.config['p'])\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "# make predictions\n",
    "best_trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val = best_trained_model(X_test)\n",
    "    loss = custom_loss(y_val, y_test, None, 'corr')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_val, y_test, '.')\n",
    "ax.text(0.6,0.1,f'Pearson corr = {-loss:.6f}',transform = ax.transAxes) # The value differs from above due to no dropout."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa33548fd232e899fabd49cae064c58c4c782c23395ee50b9799f43e0672aedf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
