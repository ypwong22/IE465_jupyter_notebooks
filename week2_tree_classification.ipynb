{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease UCI\n",
    "\n",
    "Source: https://www.kaggle.com/ronitf/heart-disease-uci\n",
    "\n",
    "Variables\n",
    "\n",
    "1. age: The person's age in years\n",
    "2. sex: The person's sex (1 = male, 0 = female)\n",
    "3. cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n",
    "4. trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n",
    "5. chol: The person's cholesterol measurement in mg/dl\n",
    "6. fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "7. restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "8. thalach: The person's maximum heart rate achieved\n",
    "9. exang: Exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n",
    "11. slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n",
    "12. ca: The number of major vessels (0-4)\n",
    "13. thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n",
    "14. target: Heart disease (0 = no, 1 = yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "1. Check missing data - none\n",
    "2. Drop the irrelevant predictors - unecessary in this case\n",
    "3. Ensure data types are correct\n",
    "3. Encode the categorical columns\n",
    "4. Normalize the continuous columns\n",
    "5. Put back the categorical and continuous columns together\n",
    "6. Split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns\n",
    "cat_cols = ['cp', 'restecg', 'slope', 'thal']\n",
    "\n",
    "# Option 1:  one-hot encode categorical variables\n",
    "def onehot_encoding(data, cat_cols):\n",
    "    df_cat   = []\n",
    "    for col in cat_cols:\n",
    "        df_cat.append(pd.get_dummies(data[col], prefix = col))\n",
    "    df_cat   = pd.concat(df_cat, axis = 1)\n",
    "    return df_cat\n",
    "df_cat = onehot_encoding(data, cat_cols)\n",
    "\n",
    "# Option 2: embedding\n",
    "def embedding(data, cat_cols):\n",
    "    cat_szs = [len(data[col].unique()) for col in cat_cols]\n",
    "    emb_szs = [(size, min(50, (size+1)//2)) for size in cat_szs]\n",
    "    selfembeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
    "    embeddingz = []\n",
    "    for i,e in enumerate(selfembeds):\n",
    "        cat = torch.tensor(data[cat_cols[i]].values, dtype = torch.int64)\n",
    "        embeddingz.append(e(cat))\n",
    "    df_cat = torch.cat(embeddingz, 1)\n",
    "    df_cat = pd.DataFrame(df_cat.detach().numpy(), index = data.index, \n",
    "                          columns = ['embeded_{}'.format(i) for i in range(df_cat.shape[1])])\n",
    "    return df_cat, emb_szs\n",
    "\n",
    "df_cat, emb_szs = embedding(data, cat_cols)\n",
    "\n",
    "# continuous columns\n",
    "df_cont = data.drop(cat_cols + ['target'], axis = 1)\n",
    "df_cont = (df_cont - df_cont.mean()) / df_cont.std()\n",
    "\n",
    "# put back together the categorical and continuous columns\n",
    "X_all = pd.concat([df_cont, df_cat], axis = 1).values\n",
    "\n",
    "# make sure the target variable is integer, in order to represent discrete classes\n",
    "y_all = data['target'].values.astype(int)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# no more need to convert to torch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE = 0.8524590163934426\n",
      "MODEL OUTPUT                Y_TEST\n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "0                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "0                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             1   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "\n",
      "42 out of 50 = 84.00% correct\n"
     ]
    }
   ],
   "source": [
    "# n_estimators - number of trees in the forest\n",
    "# criterion    - can be gini or entropy; measure the quality of a split\n",
    "# max_depth    - maximum depth of the tree\n",
    "# min_samples_split - minimum number of samples required to split an internal node\n",
    "# min_samples_leaf - minimum number of samples required to be at a leaf node\n",
    "# oob_score - out-of-bag score\n",
    "reg = RandomForestClassifier(n_estimators = 200, criterion = 'entropy',\n",
    "                             random_state = 0, max_depth = None,\n",
    "                             min_samples_split = 2, min_samples_leaf = 1, oob_score = True)\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"CE = {}\".format(reg.score(X_test, y_test)))\n",
    "y_val = reg.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "rows = 50\n",
    "correct = 0\n",
    "print(f'{\"MODEL OUTPUT\":26}  Y_TEST')\n",
    "for i in range(rows):\n",
    "    print(f'{str(y_val[i]):26} {y_test[i]:^7}')\n",
    "    if y_val[i] == y_test[i]:\n",
    "        correct += 1\n",
    "print(f'\\n{correct} out of {rows} = {100*correct/rows:.2f}% correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeRegressor(max_features='auto', random_state=209652396), DecisionTreeRegressor(max_features='auto', random_state=398764591), DecisionTreeRegressor(max_features='auto', random_state=924231285), DecisionTreeRegressor(max_features='auto', random_state=1478610112), DecisionTreeRegressor(max_features='auto', random_state=441365315), DecisionTreeRegressor(max_features='auto', random_state=1537364731), DecisionTreeRegressor(max_features='auto', random_state=192771779), DecisionTreeRegressor(max_features='auto', random_state=1491434855), DecisionTreeRegressor(max_features='auto', random_state=1819583497), DecisionTreeRegressor(max_features='auto', random_state=530702035)]\n"
     ]
    }
   ],
   "source": [
    "print(reg.estimators_[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5022959612068372\n"
     ]
    }
   ],
   "source": [
    "print(reg.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:26:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CE = 0.8524590163934426\n",
      "MODEL OUTPUT                Y_TEST\n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "0                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "0                             1   \n",
      "1                             1   \n",
      "0                             0   \n",
      "0                             1   \n",
      "1                             1   \n",
      "1                             1   \n",
      "1                             0   \n",
      "0                             0   \n",
      "1                             1   \n",
      "\n",
      "42 out of 50 = 84.00% correct\n"
     ]
    }
   ],
   "source": [
    "# n_estimators - number of boosting rounds, or number of decision trees to build\n",
    "# objective - loss function; 'binary:logistic' for binary classification, 'multi:softmax' for multiclass classification\n",
    "# max_depth - maximum depth of each regression tree\n",
    "# eta - learning rate\n",
    "# gamma - minimum loss reduction required to make a further partition on a leaf node of the tree\n",
    "# subsample - fraction of samples to be used for fitting each tree\n",
    "# colsample_bytree - fraction of features to be used for each tree\n",
    "reg = XGBClassifier(n_estimators=500, max_depth=7, eta=0.1, gamma = 0, \n",
    "                    objective = 'binary:logistic', \n",
    "                    subsample=0.6, colsample_bytree=0.5, random_state = 333,\n",
    "                    use_label_encoder = False)\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"CE = {}\".format(reg.score(X_test, y_test)))\n",
    "y_val = reg.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "rows = 50\n",
    "correct = 0\n",
    "print(f'{\"MODEL OUTPUT\":26}  Y_TEST')\n",
    "for i in range(rows):\n",
    "    print(f'{str(y_val[i]):26} {y_test[i]:^7}')\n",
    "    if y_val[i] == y_test[i]:\n",
    "        correct += 1\n",
    "print(f'\\n{correct} out of {rows} = {100*correct/rows:.2f}% correct')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa33548fd232e899fabd49cae064c58c4c782c23395ee50b9799f43e0672aedf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
